# Joseph McInerney MSc AI Project

## Part 1: Pretraining
- Preprocessing: containment, CNG, , UCCIX, Bitext 
- Training Script
    - Dependencies
    - Bash Script
    - Continued Pre-training Script
    - Distributed Training Config

## Part 2: Instruction Tuning
- API Calls to Generate LLM Instruct-Response pairs
- App for Human Annotation
- LLM Automated Annotation
- Ranking and Inter-Annotator Analysis
    - Bradley Terry Ranking and Cohen's Kappa Script
    - Dependencies
- Dataset preparationg for Google Vertext AI Translation of Dolly V2
- LoRA
    - Training script
    - Dependencies
- Statistical Analysis of Open-Ended Response Lengths
- Dataset: [HF_REPO](https://huggingface.co/datasets/jmcinern/Instruction_Ga_En_for_LoRA)

## Part 2: Human Feedback Data
- LIMA Translation and Preference Dataset Creation
- App for Native Annotation
- Dataset: [HF_REPO](https://huggingface.co/datasets/jmcinern/LIMA_ga/)
